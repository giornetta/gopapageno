# EVALUATING

To evaluate the performance of parallel parsers generated by GoPAPAGENO we chose JSON as a representative language.

> [!IMPORTANT]
> GoPAPAGENO is a tool to generate parallel parsers starting from user-provided lexicon and grammar specifications.
> It shouldn't be compared directly with production-grade specialized JSON parsers.

The benchmarks were executed on three diverse files, each approximately 180MB in size, which contain different amounts of parallel sequences and deeply nested structures. These files are the same ones used to test Associative Operator Precedence Parsing in the work that first introduced it:
- **Emojis**: the entire input is made of a single, non-nested JSON object with many key-value pairs of string type, each representing the URL to a particular emoji. This file should exhibit the maximum possible amount of parallelism.
- **Citylots**: a dataset which shows a fair amount of parallelism, as a large portion of it is made of long arrays. The elements of those arrays, however, are complex objects and not simple terminal strings.
- **Wikidata**: the input file is a single array made up of many objects with a complex structure and shorter parallel sequences. This is the file that exhibits the least amount of parallelism.

> [!NOTE]
> GoPAPAGENO allows users to set two parameters, the average token length `avg` and the parallelism factor `pf`, before running a generated parser. 
> These parameters are specific to the input file, and different values could potentially lead to very different results. 
> For these benchmarks, the following values have been decided:
> - Emojis: `avg = 8, pf = 1`
> - Citylots: `avg = 4, pf = 0.5`
> - Emojis: `avg = 4, pf = 0`

The machine used for testing is equipped with dual 2 GHz AMD EPYC 7551 32-Core processors and 503 GiB of RAM, running on Debian 6.1.99-1. Finally, all tests have been executed using Go 1.23.0 and the built-in testing and benchmarking tools provided by the Go toolchain.

## Replicating Benchmarks

To replicate the benchmarks we executed, follow these steps:

You can run the benchmarks directly in the provided Docker container:
```bash
docker run -it giornetta/gopapageno
```

Once inside the container, run the evaluation script:
```bash
./scripts/evaluate.sh
```

Alternatively, you can customize the benchmark parameters:
```bash
./scripts/evaluate.sh --bench BenchmarkParse --count 5
```

The script supports the following options:
```
-b, --bench: Specify the benchmark type (either BenchmarkParseOnly or BenchmarkParse, default: BenchmarkParseOnly)
-c, --count: Number of iterations for each benchmark (default: 10)
```

The script will benchmark the three parsing algorithms on the *Emojis*, *Citylots* and *Wikidata* datasets using all thread counts from `1` to `min(GOMAXPROCS(), 32)`.

When running the evaluation script, you should see output similar to:
```
Running benchmarks for OPP with benchmark 'BenchmarkParseOnly' and count 10...
Running benchmarks for AOPP with benchmark 'BenchmarkParseOnly' and count 10...
Running benchmarks for COPP with benchmark 'BenchmarkParseOnly' and count 10...
Generating figure BenchmarkParseOnly_json_c10.png
Generating benchstat file BenchmarkParseOnly_json_c10_compare.txt
Benchmark completed successfully!
```

The script generates several output files:
- Individual benchmark results: `benchmark_json_BenchmarkParseOnly_c10_*.txt` *(opp, aopp, copp)*
- Performance visualization: `benchmark_json_BenchmarkParseOnly_c10.png`
- Statistical comparison: `benchmark_json_BenchmarkParseOnly_c10_compare.txt`

The PNG file provides a visual representation of parsing performance across different goroutine counts, while the comparison file shows detailed statistics about the relative performance of each parsing technique.

All benchmark results are stored in the current directory for further analysis.